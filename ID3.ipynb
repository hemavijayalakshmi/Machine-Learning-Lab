{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6QSyGhoZ/E8FchCpQ5ZZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemavijayalakshmi/Machine-Learning-Lab/blob/main/ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import pandas as pd\n",
        "import math\n",
        "\n",
        "# Function to calculate the entropy of a dataset\n",
        "def entropy(data, target):\n",
        "    # Count the number of instances for each target class\n",
        "    class_counts = data[target].value_counts()  # Using the target passed to the function\n",
        "    total_instances = len(data)\n",
        "    entropy_val = 0\n",
        "    # Calculate the entropy using the formula\n",
        "    for count in class_counts:\n",
        "        prob = count / total_instances\n",
        "        entropy_val -= prob * math.log2(prob)\n",
        "    return entropy_val\n",
        "\n",
        "# Function to calculate information gain for a feature\n",
        "def information_gain(data, feature, target):\n",
        "    # Calculate the total entropy of the dataset\n",
        "    total_entropy = entropy(data, target)\n",
        "\n",
        "    # Get the unique values for the feature\n",
        "    feature_values = data[feature].unique()\n",
        "\n",
        "    # Calculate weighted entropy after splitting on the feature\n",
        "    weighted_entropy = 0\n",
        "    for value in feature_values:\n",
        "        # Subset of data where the feature has the given value\n",
        "        subset = data[data[feature] == value]\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weighted_entropy += (len(subset) / len(data)) * subset_entropy\n",
        "\n",
        "    # Information Gain = Entropy before splitting - Weighted Entropy after splitting\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# Function to build the ID3 decision tree\n",
        "def id3(data, features, target):\n",
        "    # If all instances have the same target class, return a leaf node\n",
        "    if len(data[target].unique()) == 1:\n",
        "        return data[target].iloc[0]\n",
        "\n",
        "    # If there are no more features to split on, return the most frequent target class\n",
        "    if not features:\n",
        "        return data[target].mode()[0]\n",
        "\n",
        "    # Select the feature with the highest information gain\n",
        "    best_feature = max(features, key=lambda feature: information_gain(data, feature, target))\n",
        "\n",
        "    # Create a node with the best feature\n",
        "    tree = {best_feature: {}}\n",
        "\n",
        "    # Recur for each subset of the data based on the best feature\n",
        "    for value in data[best_feature].unique():\n",
        "        # Subset of the data where the best feature has a specific value\n",
        "        subset = data[data[best_feature] == value]\n",
        "\n",
        "        # Exclude the best feature from the remaining features\n",
        "        subset_features = [f for f in features if f != best_feature]\n",
        "\n",
        "        # Recursively build the tree for the subset\n",
        "        tree[best_feature][value] = id3(subset, subset_features, target)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Load dataset from CSV\n",
        "df = pd.read_csv(\"Tennis.csv\")\n",
        "\n",
        "# Check the first few rows to verify the dataset structure\n",
        "print(df.head())\n",
        "\n",
        "# List of features (excluding the target column 'Play')\n",
        "features = ['Outlook', 'Temp', 'Humidity', 'Wind']\n",
        "target = 'Play'  # Corrected target column name\n",
        "\n",
        "# Build the decision tree\n",
        "tree = id3(df, features, target)\n",
        "\n",
        "# Print the resulting decision tree\n",
        "print(\"Decision Tree:\")\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVZj9C26yYZX",
        "outputId": "5c6190f4-8922-4413-8cfe-94d93783738b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Outlook  Temp Humidity    Wind Play\n",
            "0     Sunny   Hot     High    Weak   No\n",
            "1     Sunny   Hot     High  Strong   No\n",
            "2  Overcast   Hot     High    Weak  Yes\n",
            "3      Rain  Mild     High    Weak  Yes\n",
            "4      Rain  Cool   Normal    Weak  Yes\n",
            "Decision Tree:\n",
            "{'Outlook': {'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}, 'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}}}\n"
          ]
        }
      ]
    }
  ]
}